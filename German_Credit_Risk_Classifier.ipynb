{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa752c8-fc41-4cb4-9dfa-6c968b571154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Role Definition and Task for German Credit Risk Classifier\n",
    "\n",
    "# 1. Role: Data Science Consultant specializing in Binary Classification and Risk Assessment.\n",
    "# 2. Goal: Build a high-accuracy Classification model to predict credit risk (Good or Bad).\n",
    "# 3. Data Source: German Credit Data (available in Scikit-learn or standard datasets).\n",
    "\n",
    "# Agent, your primary task is to:\n",
    "# 1. Load the German Credit Data.\n",
    "# 2. Perform comprehensive Exploratory Data Analysis (EDA), focusing on distribution and relationships to the 'risk' target variable.\n",
    "# 3. Handle categorical variables (e.g., one-hot encoding).\n",
    "# 4. Train two classification models: Logistic Regression (as a baseline) and a more advanced model (e.g., Support Vector Machine or Random Forest).\n",
    "# 5. Evaluate the models using standard classification metrics (Accuracy, Precision, Recall, F1-Score).\n",
    "# 6. Provide clear code, comments, and a final conclusion on which model is best for financial risk assessment.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# We will use the Agent to suggest the best model and further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee442399-c678-40e2-af72-2c871155136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First 5 Rows of Data ---\n",
      "  Attribute1  Attribute2 Attribute3 Attribute4  Attribute5 Attribute6  \\\n",
      "0        A11           6        A34        A43        1169        A65   \n",
      "1        A12          48        A32        A43        5951        A61   \n",
      "2        A14          12        A34        A46        2096        A61   \n",
      "3        A11          42        A32        A42        7882        A61   \n",
      "4        A11          24        A33        A40        4870        A61   \n",
      "\n",
      "  Attribute7  Attribute8 Attribute9 Attribute10  ...  Attribute12 Attribute13  \\\n",
      "0        A75           4        A93        A101  ...         A121          67   \n",
      "1        A73           2        A92        A101  ...         A121          22   \n",
      "2        A74           2        A93        A101  ...         A121          49   \n",
      "3        A74           2        A93        A103  ...         A122          45   \n",
      "4        A73           3        A93        A101  ...         A124          53   \n",
      "\n",
      "   Attribute14 Attribute15 Attribute16  Attribute17 Attribute18  Attribute19  \\\n",
      "0         A143        A152           2         A173           1         A192   \n",
      "1         A143        A152           1         A173           1         A191   \n",
      "2         A143        A152           1         A172           2         A191   \n",
      "3         A143        A153           1         A173           2         A191   \n",
      "4         A143        A153           2         A173           2         A191   \n",
      "\n",
      "  Attribute20 Credit_Risk  \n",
      "0        A201           1  \n",
      "1        A201           2  \n",
      "2        A201           1  \n",
      "3        A201           1  \n",
      "4        A201           2  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "--- Data Information (Types and Missing Values) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 21 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Attribute1   1000 non-null   object\n",
      " 1   Attribute2   1000 non-null   int64 \n",
      " 2   Attribute3   1000 non-null   object\n",
      " 3   Attribute4   1000 non-null   object\n",
      " 4   Attribute5   1000 non-null   int64 \n",
      " 5   Attribute6   1000 non-null   object\n",
      " 6   Attribute7   1000 non-null   object\n",
      " 7   Attribute8   1000 non-null   int64 \n",
      " 8   Attribute9   1000 non-null   object\n",
      " 9   Attribute10  1000 non-null   object\n",
      " 10  Attribute11  1000 non-null   int64 \n",
      " 11  Attribute12  1000 non-null   object\n",
      " 12  Attribute13  1000 non-null   int64 \n",
      " 13  Attribute14  1000 non-null   object\n",
      " 14  Attribute15  1000 non-null   object\n",
      " 15  Attribute16  1000 non-null   int64 \n",
      " 16  Attribute17  1000 non-null   object\n",
      " 17  Attribute18  1000 non-null   int64 \n",
      " 18  Attribute19  1000 non-null   object\n",
      " 19  Attribute20  1000 non-null   object\n",
      " 20  Credit_Risk  1000 non-null   int64 \n",
      "dtypes: int64(8), object(13)\n",
      "memory usage: 164.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Agent Task 1: Load the German Credit Data and display its first 5 rows and information (info()).\n",
    "\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset \n",
    "# The German Credit Data is readily available through the UCI Machine Learning Repository\n",
    "try:\n",
    "    german_credit = fetch_ucirepo(id=144)\n",
    "    \n",
    "    # Data (features) and Target variable\n",
    "    X = german_credit.data.features\n",
    "    y = german_credit.data.targets\n",
    "    \n",
    "    # Combine for easier EDA\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Rename target column for clarity (0: Good Risk, 1: Bad Risk)\n",
    "    # The original target column is often named 'class' or similar. We will rename the last column.\n",
    "    df.columns = list(df.columns[:-1]) + ['Credit_Risk']\n",
    "    \n",
    "    print(\"--- First 5 Rows of Data ---\")\n",
    "    print(df.head())\n",
    "    print(\"\\n--- Data Information (Types and Missing Values) ---\")\n",
    "    df.info()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data. Trying alternative source (if applicable) or check library installation: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d42960-5880-48b6-84fb-911d1890a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Credit Risk Distribution (Before Renaming) ---\n",
      "Credit_Risk\n",
      "1    700\n",
      "2    300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- Remapped Credit Risk Distribution (0: Good Risk, 1: Bad Risk) ---\n",
      "Credit_Risk\n",
      "0    700\n",
      "1    300\n",
      "Name: count, dtype: int64\n",
      "Total Bad Risk (1): 300 (0.30%)\n",
      "Total Good Risk (0): 700 (0.70%)\n",
      "\n",
      "--- Processed Data Shape ---\n",
      "Original Shape: (1000, 21)\n",
      "Processed Shape (after One-Hot Encoding): (1000, 49)\n"
     ]
    }
   ],
   "source": [
    "# Agent Task 2: Data Preprocessing and Target Variable Analysis\n",
    "\n",
    "# 1. Analyze the distribution of the target variable (Credit_Risk).\n",
    "# 2. Convert the 'object' type categorical columns into numerical form using One-Hot Encoding (pd.get_dummies).\n",
    "# 3. Rename the target variable's values to be binary (0 and 1) instead of the current (1 and 2), where 1 = Good Risk and 0 = Bad Risk, as is standard in ML.\n",
    "# 4. Display the shapes of the original and processed DataFrame.\n",
    "\n",
    "# Analyze Target Distribution\n",
    "print(\"--- Credit Risk Distribution (Before Renaming) ---\")\n",
    "print(df['Credit_Risk'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "# The original dataset defines 1 as Good Risk and 2 as Bad Risk. We remap 1 -> 1 (Good) and 2 -> 0 (Bad) for standard binary classification.\n",
    "# NOTE: Based on UCI documentation, the original classes are 1 (Good) and 2 (Bad). Let's remap 1 to 0 (Good) and 2 to 1 (Bad) to align with standard ML practice where 1 often represents the event of interest (Bad Risk).\n",
    "# Let's check the distribution again after remapping to 0 and 1\n",
    "df['Credit_Risk'] = df['Credit_Risk'].replace({1: 0, 2: 1})\n",
    "\n",
    "print(\"--- Remapped Credit Risk Distribution (0: Good Risk, 1: Bad Risk) ---\")\n",
    "print(df['Credit_Risk'].value_counts())\n",
    "print(f\"Total Bad Risk (1): {df['Credit_Risk'].value_counts()[1]} ({df['Credit_Risk'].value_counts(normalize=True)[1]:.2f}%)\")\n",
    "print(f\"Total Good Risk (0): {df['Credit_Risk'].value_counts()[0]} ({df['Credit_Risk'].value_counts(normalize=True)[0]:.2f}%)\")\n",
    "\n",
    "# Handle Categorical Features using One-Hot Encoding\n",
    "df_processed = pd.get_dummies(df, columns=df.select_dtypes(include=['object']).columns, drop_first=True)\n",
    "\n",
    "print(\"\\n--- Processed Data Shape ---\")\n",
    "print(f\"Original Shape: {df.shape}\")\n",
    "print(f\"Processed Shape (after One-Hot Encoding): {df_processed.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f2ef19-bc6c-4614-898f-81ffa601cd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Splitting Complete ---\n",
      "Training set size: 800 samples\n",
      "Testing set size: 200 samples\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation: Logistic Regression ---\n",
      "Accuracy: 0.7850\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       140\n",
      "           1       0.68      0.53      0.60        60\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.75      0.71      0.73       200\n",
      "weighted avg       0.78      0.79      0.78       200\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluation: Random Forest Classifier ---\n",
      "Accuracy: 0.7700\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85       140\n",
      "           1       0.72      0.38      0.50        60\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.75      0.66      0.68       200\n",
      "weighted avg       0.76      0.77      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agent Task 3: Model Training and Evaluation\n",
    "\n",
    "# 1. Separate features (X) and target (y).\n",
    "# 2. Split the data into training and testing sets (80% train, 20% test).\n",
    "# 3. Train two models: Logistic Regression and Random Forest Classifier.\n",
    "# 4. Evaluate both models and print a detailed classification report for each.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Separate Features and Target\n",
    "X = df_processed.drop('Credit_Risk', axis=1)\n",
    "y = df_processed['Credit_Risk']\n",
    "\n",
    "# 2. Split Data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# Note: stratify=y ensures the 70/30 imbalance is maintained in both train and test sets.\n",
    "\n",
    "print(\"--- Data Splitting Complete ---\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# 3a. Train Logistic Regression (Baseline Model)\n",
    "logreg_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "# 4a. Evaluate Logistic Regression\n",
    "print(\"--- Evaluation: Logistic Regression ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_logreg):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# 3b. Train Random Forest Classifier (Advanced Model)\n",
    "# Random Forest often handles complex relationships and is good with high-dimensional data (after one-hot encoding).\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 4b. Evaluate Random Forest\n",
    "print(\"--- Evaluation: Random Forest Classifier ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b97c493-eee6-4c9e-a409-20800fcde18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL CLASSIFICATION CONCLUSION ---\n",
      "1. Data Imbalance: The data is highly imbalanced (70% Good Risk, 30% Bad Risk).\n",
      "2. Evaluation Focus: Given the financial context, the primary focus must be on identifying 'Bad Risk' (Class 1) correctly, which means prioritizing Recall and F1-Score for Class 1.\n",
      "\n",
      "\n",
      "Logistic Regression F1-Score (Bad Risk): 0.6\n",
      "Random Forest F1-Score (Bad Risk): 0.5\n",
      "\n",
      "CONCLUSION: The Logistic Regression model demonstrates better overall performance, especially in terms of F1-Score for the minority class (Bad Risk). Although Recall for Bad Risk (53%) is still a concern, it is the superior model between the two tested. Further optimization (e.g., using techniques like SMOTE or cost-sensitive learning) is recommended to improve the identification of high-risk customers.\n"
     ]
    }
   ],
   "source": [
    "# Agent Task 4: Final Conclusion and Documentation Summary\n",
    "\n",
    "print(\"--- FINAL CLASSIFICATION CONCLUSION ---\")\n",
    "print(\"1. Data Imbalance: The data is highly imbalanced (70% Good Risk, 30% Bad Risk).\")\n",
    "print(\"2. Evaluation Focus: Given the financial context, the primary focus must be on identifying 'Bad Risk' (Class 1) correctly, which means prioritizing Recall and F1-Score for Class 1.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Comparative Analysis\n",
    "logreg_f1 = 0.60\n",
    "rf_f1 = 0.50\n",
    "\n",
    "print(f\"Logistic Regression F1-Score (Bad Risk): {logreg_f1}\")\n",
    "print(f\"Random Forest F1-Score (Bad Risk): {rf_f1}\")\n",
    "\n",
    "if logreg_f1 > rf_f1:\n",
    "    print(\"\\nCONCLUSION: The Logistic Regression model demonstrates better overall performance, especially in terms of F1-Score for the minority class (Bad Risk). Although Recall for Bad Risk (53%) is still a concern, it is the superior model between the two tested. Further optimization (e.g., using techniques like SMOTE or cost-sensitive learning) is recommended to improve the identification of high-risk customers.\")\n",
    "else:\n",
    "    print(\"\\nCONCLUSION: The Random Forest Classifier demonstrates better overall performance, particularly in managing the complexity introduced by One-Hot Encoding. This model is recommended for deployment, but further tuning is needed to increase the Recall for the Bad Risk class.\")\n",
    "\n",
    "# Save the Notebook for GitHub\n",
    "# Ensure you save your Notebook as 'German_Credit_Risk_Classifier.ipynb' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689f3e5-eb7c-43fa-be12-5f03ceb66f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
